---
title: "Untitled"
output: html
date: "2024-11-24"
---
---
title: "HW12: Logistisc Regression, LMM "
format: pdf
editor: visual
---
## Question 1: CRP and Logistic Regression


### Question A: i

\[
\log\left(\frac{\pi}{1-\pi}\right) \sim \mu + \beta_{\text{age}} X_{\text{age}}
\]

\[
\log\left(\frac{\pi}{1-\pi}\right) \sim \mu + \beta_{\text{age}} X_{\text{age}} + \beta_{\text{sex}} X_{\text{sex}}
\]

\[
\log\left(\frac{\pi}{1-\pi}\right) \sim \mu + \beta_{\text{age}} X_{\text{age}} + \beta_{\text{sex}} X_{\text{sex}} + \beta_{\text{age} \times \text{sex}} X_{\text{age} \times \text{sex}}
\]


### Question A: ii

```{r}
install.packages("psycModel")
# converts pi to odds

prob2odds <- function(p_i) {
  odds <- p_i / (1 - p_i)
  return(odds)
}

#converts odds to Ni

logit <- function(p_i) {
  eta_i <- log(p_i / (1 - p_i))
  return(eta_i)
}

#converts Ni back to pi

inv.logit <- function(eta_i) {
  p_i <- exp(eta_i) / (1 + exp(eta_i))
  return(p_i)
}

```

### Quesstion A: iii

Log model

For the logistic regression model:

\[
\log\left(\frac{p_i}{1 - p_i}\right) = \beta_0 + \beta_{\text{gender}} \cdot \text{gender}_i + \beta_{\text{age}} \cdot \text{age}_i
\]

Where gender is coded as 0 for male and 1 for female.

#### Odds for Male (gender = 0):

\[
\log\left(\frac{p_{\text{male}}}{1 - p_{\text{male}}}\right) = \beta_0 + \beta_{\text{age}} \cdot 30
\]

Exponentiating both sides:

\[
\frac{p_{\text{male}}}{1 - p_{\text{male}}} = e^{\beta_0 + \beta_{\text{age}} \cdot 30}
\]

#### Odds for Female (gender = 1):

\[
\log\left(\frac{p_{\text{female}}}{1 - p_{\text{female}}}\right) = \beta_0 + \beta_{\text{gender}} + \beta_{\text{age}} \cdot 30
\]

Exponentiating both sides:

\[
\frac{p_{\text{female}}}{1 - p_{\text{female}}} = e^{\beta_0 + \beta_{\text{gender}} + \beta_{\text{age}} \cdot 30}
\]

Expressing female odds in terms of male odds:

\[
\frac{p_{\text{female}}}{1 - p_{\text{female}}} = e^{\beta_{\text{gender}}} \cdot \frac{p_{\text{male}}}{1 - p_{\text{male}}}
\]

And thus:

\[
\text{odds(female, 30)} = e^{\beta_{\text{gender}}} \times \text{odds(male, 30)}
\]


And thus:

odds(female,30) = $e^{\beta_{gender}}$ X odds(male,30)

### Question B

```{r}
#read in crp and add new status column
crp <- read.csv("crp.csv")
crp$status <- ifelse(crp$CRP > 4.5,1,0)

#fitting new models

fit.a <- glm(status ~ 1 + Age, data = crp, family = "binomial")
fit.as <- glm(status ~ 1 + Age + Gender, data = crp, family = "binomial")
fit.axs <- glm(status ~ 1 + Age + Gender + Age*Gender, data = crp, family = "binomial")

```

### Question B:i

```{r}
#takin the age coeficent from the model above
beta_age <- coef(fit.a)["Age"]

#mulitply by 10 to see how it changes for a decade
odds_ratio_decade <- exp(beta_age * 10)
print(odds_ratio_decade)

```

### Question B: ii

```{r}

ageCI<- confint(fit.a, level = 0.95)
cat("Lower CI:",ageCI["Age",1], "\n", "Upper CI:", ageCI["Age",2] )

```

### Question B:iii

```{r}
beta_sex <- coef(fit.as)["Gender"]
beta_age <- coef(fit.as)["Age"]

# Log-odds for a 32-year-old female
female_32 <- coef(fit.as)["(Intercept)"] + beta_age * 32 + beta_sex

# Log-odds for a 57-year-old female
female_57 <- coef(fit.as)["(Intercept)"] + beta_age * 57 + beta_sex

# Odds ratios
odds_ratio_32 <- exp(female_32)
odds_ratio_57 <- exp(female_57)

# Print results
print(odds_ratio_32)
print(odds_ratio_57)

# Odds ratio between the two ages
oddsratio_tots <- odds_ratio_57 / odds_ratio_32
print(oddsratio_tots)
print(summary(fit.as))

```

### Question B: iv

```{r}

beta_sex <- coef(fit.axs)[3]
beta_age <- coef(fit.axs)[2]
beta_int <- coef(fit.axs)[4]

# Log-odds for a 32-year-old female
female_32 <- coef(fit.axs)[1] + beta_age * 32 + beta_sex

# Log-odds for a 57-year-old female
female_57 <- coef(fit.as)[1] + beta_age * 57 + beta_sex

# Odds ratios
odds_ratio_32 <- exp(female_32)
odds_ratio_57 <- exp(female_57) 

# Print results
print(odds_ratio_32)
print(odds_ratio_57)
oddsratio.tots <- odds_ratio_57 / odds_ratio_32
print(oddsratio.tots)
print(summary(fit.axs))


```

### Question B: V

**If the odds ratio of an effect is not significant, what value must its confidence interval cover?**

It must cover 1, an odds ratio of 1 indicated that there is no difference between the groups being compared so the predictor has no effect on the outcome. CI gives the range that the true odds is within that range. So we cant rule out the possibility of it not being statistically significant

### Question B: vi

```{r}
# df for the newdata to fit into the model
highcrpmale <- data.frame(Age = 32, Gender = 0)

#predicting base on the model + new data
male.pred <- predict.glm(fit.as,newdata = highcrpmale, type = "response" )

print(male.pred)


```

### Question B: vii

```{r}
library(ggplot2)

# modeling the first one

prediction <- predict.glm(fit.a, type = "response")

ggplot(crp, aes(x = Age, y = status))+ 
  geom_point(alpha = 0.5)+
  stat_smooth(method = "glm", se = TRUE, method.args = list(family = binomial))



# tring to model the 2nd one

#factor gender for plot
crp$Gender <- as.factor(crp$Gender)

# Refit the model with Gender as a factor
fit.as <- glm(status ~ 1 + Age + Gender, data = crp, family = "binomial")

#data frame for prediction probables
new_data <- expand.grid(
  Age = seq(min(crp$Age), max(crp$Age), length.out = 100),
  Gender = levels(crp$Gender)
)

# Predict probabilities
new_data$predicted_prob <- predict(fit.as, newdata = new_data, type = "response")

# Plot 
ggplot(new_data, aes(x = Age, y = predicted_prob, color = Gender)) +
  geom_jitter(data = crp, aes(x = Age, y = status, color = Gender),height = 0.01, width = 0.05, size = 1)+
  geom_line(size = 1) +
  labs(title = "Predicted Probabilities of Status by Age and Gender",
       x = "Age",
       y = "Predicted Probability",
       color = "Gender") +
  theme_minimal()


```

### Question C:

Sig lect 21 page 8

-   models, one with int vs age, one int vs add same as hw11 b ii

```{r}
# log likes of the different models
age <- logLik(fit.a)
age.sex <- logLik(fit.as)
agexsex <- logLik(fit.axs)

#filling in above values for the 
LRT <- 2*(agexsex - age.sex)
pvals <- (1- pchisq(LRT, df = 1))

LTR2 <- 2*(agexsex - age)
pvals2 <- (1- pchisq(LTR2, df = 2))


#model comparisons
chi <- anova(fit.axs, test = "Chisq")
lts.test <- anova(fit.axs, fit.as, test = "LRT")

print(chi[5])
print(lts.test[5])



```

### Question D: i

The beta would have to be extremely large to have a prediction probability for high CRP. anything not high isnt going to change the model much thus not a good predictor

### Question D: ii

The very drastic predictor makes it tough because there is little flexibility within the model and can lead to information that is not accurately portraying the situation. We know that not all people are actually going to be +200K for the high CRP but because it fits that situation in this specific model. So technically accurate but wrong for the actual use, need to be caredful when picking predicots

### Question D:iii

```{r}

crp$cutoff <- ifelse(crp$CRP > 15,1,0)

print(stem(crp$cutoff))
print(stem(crp$status))

# print the different stem plots and talk about the proportion of data we're seeing. So we don't want to be cutting off information available etc

```

** why choosing such an extreme level might be less preferable in this
 case**
 
Risk of loosing information and false display of the data. We can see in the stem plots that the difference in choosing a cutoff is going to drastically reduce significance. If the values are too dratic than some (if not all) informatin will be lost. That defeats the purpose of presenting data

### Question E

Make 2 models and make sure they are using status as the pred term using lm and glm anova(model, test LRT)

```{r}

# models to compare
fit.axs <- glm(status ~ 1 + Age + Gender + Age*Gender, data = crp, family = "binomial")
fitlm.axs <- lm(status ~ 1 + Age + Gender + Age*Gender, data = crp)

# seq anova
anova(fitlm.axs, fit.axs)

#other steps done in question c
lm.agexsex <- logLik(fitlm.axs)
LRT <- 2*(lm.agexsex - agexsex)
pvals3 <- (1- pchisq(LRT, df = 0))


#model comparisons
chi2 <- anova(fitlm.axs, test = "Chisq")
lts.test2 <- anova(fitlm.axs, fit.axs, test = "LRT")

print(chi2)
print(chi)
print(lts.test)
print(lts.test2)


```

**Comment on the similarity of the logistic model vs the LPM approximation, and whether the effect estimates have the same meaning**

Log (glm) is better for a more binary outcome while lm is suited for a more continuous variable type. the linear coefs are direct changes while glm is log-odds that are exponentized to get the odds ratio. The estimates are going to be different mathmatically but the interpretion should fit for each situation. You should always change model type to best fit the data your dealing with

```{r}
#html_to_pdf(file.path = "/Users/ivory/Downloads/GitHub/720/test.html", scale  =1)
 pagedown::chrome_print("/Users/ivory/Downloads/GitHub/720/HW12.html")
```

